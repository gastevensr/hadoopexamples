/**
 * 
 */
package com.gasr.hadoopbook.chapter19;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

/**
 * @author alex
 *
 */
public class MaxTemperatureSparkJava8 {
	
	public static void main(String[] args) {
		if (args.length != 2) {
			System.err.println("Usage: MaxTemperatureSpark <input path> <output path>");
			System.exit(-1);
		}
		SparkConf conf = new SparkConf();
		JavaSparkContext sc = new JavaSparkContext("local", "MaxTemperatureSpark", conf);

		/*Function<String, String[]> split = line -> line.split("\t");
		Function<String[],Boolean> filter = line -> !line[1].equals("9999") && line[2].matches("[01459]");
		PairFunction<String[], Integer, Integer> tuples = line -> new Tuple2<Integer, Integer>(Integer.parseInt(line[0]), Integer.parseInt(line[1]));
		Function2<Integer, Integer, Integer> maxTemps = (i1, i2) -> Math.max(i1, i2);
		sc.textFile(args[0])
		.map(split)
		.filter(filter)
		.mapToPair(tuples)
		.reduceByKey(maxTemps)
		.saveAsTextFile(args[1]);*/
		sc.textFile(args[0])
		.map(line -> line.split("\t"))
		.filter(line -> !line[1].equals("9999") && line[2].matches("[01459]"))
		.mapToPair(line -> new Tuple2<Integer, Integer>(Integer.parseInt(line[0]), Integer.parseInt(line[1])))
		.reduceByKey((i1, i2) -> Math.max(i1, i2))
		.saveAsTextFile(args[1]);
		
		sc.close();
	}
}
